{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load features and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create relevant directories\n",
    "import os\n",
    "features_dir = os.path.join(os.pardir, 'data', 'processed', 'Onehotfeatures.pkl')\n",
    "target_dir = os.path.join(os.pardir, 'data', 'processed', 'target.npy')\n",
    "\n",
    "# load features\n",
    "import pickle\n",
    "features = pickle.load( open( features_dir, 'rb' ) )\n",
    "\n",
    "# load associated targets\n",
    "from numpy import load\n",
    "y = load(target_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose features and prepare data for scikit-learn prototyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# keep features of interest\n",
    "imp_feature = ['User_ID', 'Product_ID', 'Gender_Prod_cat123']\n",
    "# imp_feature = ['User_ID', 'Product_ID', 'Gender', 'Prod_cat123']\n",
    "# only keep corresponding features\n",
    "X_features = tuple(features.get(f, 'Feature not present') for f in imp_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((550071, 9996), scipy.sparse.coo.coo_matrix)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.sparse import hstack\n",
    "X = hstack( X_features )\n",
    "X.shape, type(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Regression Tree (GBRT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As opposed to Random Forest, GBRT does `'best'` split for each node of each tree. `'best'` finds the optimal feature and threshold for each new node. However, we have control over the sample size (number of training instances to include at each new tree) which will add randomness to this sequential process (see '`subsample`')\n",
    "\n",
    "There is a `'learning_rate'` to control quantity of residuals to cancel out from one tree to another.\n",
    "`'warm_start'` is here so that when fit() is called in a loop, it doesn't start over. Instead, it builds up from the last tree created.\n",
    "\n",
    "In summary, here are the hyperparameters:\n",
    "1. **`n_estimators`**, number of sequential trees, should be higher if learning rate low (generally tuned by cv)\n",
    "2. **`learning_rate`**, controls the amount of change in your target. The lower the better. Default value of 0.1 is a good start\n",
    "3. **`subsample`**, select randomly a fraction of training samples for each tree. It trades a higher bias for a lower variance. This is called Stochastic Gradient Boosting. Typical value is 0.8\n",
    "\n",
    "[Sklearn Gradient Boosting Regressor](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# metric needed to stop loop\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# train/ val split, test_size=0.25 by default\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create GBRT and run it in a loop to find the optimal number of estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gradient Boosting for decision trees where residuals are used as a target for the next tree\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "# init parameter: input a model to get started, initial tree only (Optional)\n",
    "gb_regtree = GradientBoostingRegressor(n_estimators = 100, learning_rate = 0.01,\\\n",
    "                                       warm_start = True, subsample = 0.8,\\\n",
    "                                       min_samples_leaf = 5, max_depth = 925,\\\n",
    "                                       random_state = 29, verbose = 1)\n",
    "# run the entire \n",
    "# gb_regtree.fit(X,y)\n",
    "\n",
    "# monitor the error\n",
    "min_val_error = float('Inf')\n",
    "# counter when error goes up again\n",
    "error_going_up = 0\n",
    "# must use the same train/val data, split just once\n",
    "# create up to 500 trees, TO RERUN WITH 1000\n",
    "for n_repetition in range(1, 1000):\n",
    "    # set number of estimators\n",
    "    gb_regtree.n_estimators = n_repetition\n",
    "\n",
    "    # train model\n",
    "    gb_regtree.fit(X_train, y_train)\n",
    "    \n",
    "    # predict on validation set\n",
    "    y_val_pred = gb_regtree.predict(X_val)\n",
    "    \n",
    "    # assess error (MSE)\n",
    "    val_score = mean_squared_error(y_val, y_val_pred)\n",
    "    \n",
    "    # decide to continue or stop loop\n",
    "    if val_score < min_val_error:\n",
    "        min_val_error = val_score\n",
    "        error_going_up = 0\n",
    "    else:\n",
    "        error_going_up += 1\n",
    "        # stop when error is higher than minimum 5 times in a row\n",
    "        if error_going_up == 5:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance result on cross-validation (1 fold here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# result after optimization\n",
    "print('MSE (mean cross-validation) = {:.4f}'.format(min_val_error) )\n",
    "print('RMSE (mean cross-validation) = {:.4f}'.format(np.sqrt(min_val_error)) )\n",
    "\n",
    "# Recent results:\n",
    "# RMSE (mean cross-validation) = 2596.6717, learning_rate=0.1 and subsample = 0.25\n",
    "# RMSE (mean cross-validation) = 2528.0135, learning_rate=0.05 and subsample = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SAVE MODEL\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(gb_regtree, './../models/GBRT_Model.pkl')\n",
    "\n",
    "# example to load model\n",
    "# gb_regtree = joblib.load('GBRT_Model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the error as a function of number of estimators. Error should flatten out toward the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# recover all cross validation errors\n",
    "errors_val = [np.sqrt(mean_squared_error(y_s, y_val_pred)) for y_s in gb_regtree.staged_predict(X_val)]\n",
    "\n",
    "# keep matplotlib interactive\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "# use ggplot style\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# error vs number of trees (iteration)\n",
    "_, axgbrt = plt.subplots()\n",
    "axgbrt.plot(range(gb_regtree.n_estimators),errors_val)\n",
    "# add title and axes labels\n",
    "axgbrt.set_title('Error as a function of estimators')\n",
    "axgbrt.set_xlabel('Number of estimators (tree)')\n",
    "axgbrt.set_ylabel('MSE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run model on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Properly import custom modules for data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# expose the 'utils' directory as one where we can import modules\n",
    "# here utils is one directory up from notebooks\n",
    "utils_dir = os.path.join(os.getcwd(), os.pardir)\n",
    "sys.path.append(utils_dir)\n",
    "\n",
    "# import custom package fextract made for this project\n",
    "from utils import fextract as ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read test set in memory, recover encoders from file and derive one-hot encoded features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import importlib\n",
    "\n",
    "test_dir = os.path.join(os.pardir, 'data', 'raw','test_HujdGe7', 'test.csv')\n",
    "df_test = pd.read_csv(test_dir)\n",
    "\n",
    "# Load encoders\n",
    "OneHotencoder_dir = os.path.join(os.pardir, 'data', 'Onehotencoders.pkl')\n",
    "encoders = pickle.load( open( OneHotencoder_dir, 'rb' ) )\n",
    "Catencoder_dir = os.path.join(os.pardir, 'data', 'Category_encoders.pkl')\n",
    "catcoders = pickle.load( open( Catencoder_dir, 'rb' ) )\n",
    "\n",
    "# reload is necessary if one makes changes in fextract. Indeed modules are loaded once only, this forces a reload.\n",
    "importlib.reload(ft)\n",
    "\n",
    "# get one-hot encoded features and their names\n",
    "features_test = ft.prepare_Data(df_test, (catcoders, encoders))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select necessary features (must match your feature model obviously)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_features_test = tuple(features_test.get(f, 'Feature not present') for f in imp_feature)\n",
    "X_test = hstack( X_features_test )\n",
    "# check shape\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions and save them to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = gb_regtree.predict(X_test)\n",
    "\n",
    "# format result save to csv for submission\n",
    "df_results = df_test.loc[:,('User_ID','Product_ID')]\n",
    "df_results['Purchase'] = y_pred_test.reshape(-1,1)\n",
    "submission_dir = os.path.join(os.pardir,'submissions', 'Submission_GBRT.csv')\n",
    "df_results.to_csv(submission_dir, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
