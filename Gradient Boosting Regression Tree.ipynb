{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load features and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load features\n",
    "import pickle\n",
    "features = pickle.load( open( \"Onehotfeatures.pkl\", \"rb\" ) )\n",
    "\n",
    "# load associated targets\n",
    "from numpy import load\n",
    "y = load('target.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose features and prepare data for scikit-learn prototyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# keep features of interest\n",
    "imp_feature = ['User_ID', 'Product_ID', 'Gender_Prod_cat123']\n",
    "# imp_feature = ['User_ID', 'Product_ID', 'Gender', 'Prod_cat123']\n",
    "# only keep corresponding features\n",
    "X_features = tuple(f[0] for f in features if f[1] in imp_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "X = hstack( X_features )\n",
    "X.shape, type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load features and targets\n",
    "\n",
    "# load features\n",
    "import pickle\n",
    "features = pickle.load( open( \"Onehotfeatures.pkl\", \"rb\" ) )\n",
    "\n",
    "# load associated targets\n",
    "from numpy import load\n",
    "y = load('target.npy')\n",
    "\n",
    "Choose features and prepare data for scikit-learn prototyping\n",
    "\n",
    "# keep features of interest\n",
    "imp_feature = ['User_ID', 'Product_ID', 'Gender_Prod_cat123']\n",
    "# imp_feature = ['User_ID', 'Product_ID', 'Gender', 'Prod_cat123']\n",
    "# only keep corresponding features\n",
    "X_features = tuple(f[0] for f in features if f[1] in imp_feature)\n",
    "\n",
    "from scipy.sparse import hstack\n",
    "X = hstack( X_features )\n",
    "X.shape, type(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Regression Tree (GBRT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As opposed to Random Forest, GBRT does `'best'` split for each node of each tree. `'best'` finds the optimal feature and threshold for each new node. However, we have control over the sample size (number of training instances to include at each new tree) which will add randomness to this sequential process (see '`subsample`')\n",
    "\n",
    "There is a `'learning_rate'` to control quantity of residuals to cancel out from one tree to another.\n",
    "`'warm_start'` is here so that when fit() is called in a loop, it doesn't start over. Instead, it builds up from the last tree created.\n",
    "\n",
    "In summary, here are the hyperparameters:\n",
    "1. **`n_estimators`**, number of sequential trees, should be higher if learning rate low (generally tuned by cv)\n",
    "2. **`learning_rate`**, controls the amount of change in your target. The lower the better. Default value of 0.1 is a good start\n",
    "3. **`subsample`**, select randomly a fraction of training samples for each tree. It trades a higher bias for a lower variance. This is called Stochastic Gradient Boosting. Typical value is 0.8\n",
    "\n",
    "[Sklearn Gradient Boosting Regressor](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create GBRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gradient Boosting for decision trees where residuals are used as a target for the next tree\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "# init parameter: input a model to get started, initial tree only (Optional)\n",
    "gb_regtree = GradientBoostingRegressor(n_estimators = 100, learning_rate = 0.05,\\\n",
    "                                       warm_start = True, subsample = 0.8,\\\n",
    "                                       min_samples_leaf = 5, max_depth = 925,\\\n",
    "                                       random_state = 29, verbose = 1)\n",
    "# run the entire \n",
    "# gb_regtree.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# metric needed to stop loop\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# train/ val split, test_size=0.25 by default\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run in a loop to find the optimal number of estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# monitor the error\n",
    "min_val_error = float('Inf')\n",
    "# counter when error goes up again\n",
    "error_going_up = 0\n",
    "# must use the same train/val data, split just once\n",
    "# create up to 500 trees\n",
    "for n_repetition in range(1, 500):\n",
    "    # set number of estimators\n",
    "    gb_regtree.n_estimators = n_repetition\n",
    "\n",
    "    # train model\n",
    "    gb_regtree.fit(X_train, y_train)\n",
    "    \n",
    "    # predict on validation set\n",
    "    y_val_pred = gb_regtree.predict(X_val)\n",
    "    \n",
    "    # assess error (MSE)\n",
    "    val_score = mean_squared_error(y_val, y_val_pred)\n",
    "    \n",
    "    # decide to continue or stop loop\n",
    "    if val_score < min_val_error:\n",
    "        min_val_error = val_score\n",
    "        error_going_up = 0\n",
    "    else:\n",
    "        error_going_up += 1\n",
    "        # stop when error is going up 5 times consecutively\n",
    "        if error_going_up == 5:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance result on cross-validation (1 fold here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# result after optimization\n",
    "print('MSE (mean cross-validation) = {:.4f}'.format(min_val_error) )\n",
    "print('RMSE (mean cross-validation) = {:.4f}'.format(np.sqrt(min_val_error)) )\n",
    "\n",
    "# Recent results:\n",
    "# RMSE (mean cross-validation) = 2596.6717, subsample = 0.25\n",
    "# RMSE (mean cross-validation) = 2528.0135, learning_rate=0.05 and subsample = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE MODEL\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(gb_regtree, 'GBRT_Model.pkl')\n",
    "\n",
    "# example to load model\n",
    "# gb_regtree = joblib.load('GBRT_Model.pkl') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the error as a function of number of estimators. Error should flatten out toward the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recover all cross validation errors\n",
    "errors_val = [mean_squared_error(y_s, y_val_pred) for y_s in gb_regtree.staged_predict(X_val)]\n",
    "\n",
    "# keep matplotlib interactive\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "# use ggplot style\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# error vs number of trees (iteration)\n",
    "_, axgbrt = plt.subplots()\n",
    "axgbrt.plot(range(gb_regtree.n_estimators),errors_val)\n",
    "# add title and axes labels\n",
    "axgbrt.set_title('Error as a function of estimators')\n",
    "axgbrt.set_xlabel('Number of estimators (tree)')\n",
    "axgbrt.set_ylabel('MSE')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
