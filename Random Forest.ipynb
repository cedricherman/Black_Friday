{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load features and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load features\n",
    "import pickle\n",
    "features = pickle.load( open( \"Onehotfeatures.pkl\", \"rb\" ) )\n",
    "\n",
    "# load associated targets\n",
    "from numpy import load\n",
    "y = load('target.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose features and prepare data for scikit-learn prototyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# keep features of interest\n",
    "imp_feature = ['User_ID', 'Product_ID', 'Gender_Prod_cat123']\n",
    "# imp_feature = ['User_ID', 'Product_ID', 'Gender', 'Prod_cat123']\n",
    "# only keep corresponding features\n",
    "X_features = tuple(f[0] for f in features if f[1] in imp_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((550068, 9993), scipy.sparse.coo.coo_matrix)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.sparse import hstack\n",
    "X = hstack( X_features )\n",
    "X.shape, type(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest - Grid search (Takes too long on my machine, think days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DEFINE PARAMETERS VALUES FOR GRID SEARCH\n",
    "import numpy as np\n",
    "depth_arr = np.array([ 10**x for x in range(2,4+1)])\n",
    "# depth_arr = np.array([ x for x in range(100,1000+100, 100)])\n",
    "min_leaf_arr = np.array([ x+1 for x in range(0, 100+10, 10)])\n",
    "min_leaf_arr[0]=1\n",
    "# min_leaf_arr = np.array([ x for x in range(0, 20+1, 1)])\n",
    "# min_leaf_arr = min_leaf_arr[1:]\n",
    "est_arr = np.array([ 200, 500, 3000, 5000 ])\n",
    "\n",
    "print(est_arr)\n",
    "print(depth_arr)\n",
    "print(min_leaf_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Random forest are bags of decision trees, running this grid will take DAYS on a quad-core i7!\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rfr = RandomForestRegressor(n_estimators=10,\\\n",
    "                            min_samples_leaf = 5, max_depth = 900,\\\n",
    "                            n_jobs=-1, random_state=29, bootstrap = True, verbose = 1)\n",
    "\n",
    "# Grid search on max_depth, min_samples_leaf\n",
    "from sklearn.model_selection import GridSearchCV, ShuffleSplit\n",
    "# n_splits is the number of times you split data after shuffling\n",
    "cv = ShuffleSplit(n_splits=1, test_size=1/5, random_state=4)\n",
    "# cv could be a fixed number of partitions but there would be no shuffling in that case\n",
    "# it will just rotate on partitions (k-1) parts and 1 part for cross-val\n",
    "param_grid_rfr = [{'max_depth': depth_arr,\\\n",
    "                   'min_samples_leaf': min_leaf_arr,\\\n",
    "                  'n_estimators': est_arr}]\n",
    "rfr_grid = GridSearchCV(rfr, param_grid_rfr, cv=cv,\\\n",
    "                          scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "# run grid search\n",
    "# rfr_grid.fit(X,y)\n",
    "\n",
    "# best of mean test score\n",
    "print( 'MSE, best param, mean cross-val = {:.4f}'.format(-rfr_grid.best_score_) )\n",
    "print( 'RMSE, best param, mean cross-val = {:.4f}'.format(np.sqrt(-rfr_grid.best_score_)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest, single model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, BaggingRegressor\n",
    "# Each tree has as many samples as the original training set (cannot change that setting\n",
    "# unless you use Bagging class in sklearn), bootstrap adds randomness\n",
    "# Each tree is build by splitting node randomly. This ensures each tree is different\n",
    "# (cannot change that setting unless you use Bagging class then decision tree default\n",
    "# is 'best' for node spiltting)\n",
    "rfr = RandomForestRegressor(n_estimators = 500,\\\n",
    "                            min_samples_leaf = 5, max_depth = 900,\\\n",
    "                            n_jobs = -1, random_state = 29, bootstrap = True, verbose = 1)\n",
    "# rfr = BaggingRegressor(\n",
    "#                 DecisionTreeRegressor(spiltter='best', min_samples_leaf = 5, max_depth = 900)\n",
    "#                 n_estimators = 10, max_samples = 1.0,\\\n",
    "#                 n_jobs = -1, , bootstrap = True, random_state = 29, verbose = 1)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_validate, ShuffleSplit\n",
    "# n_splits is the number of times you split data after shuffling\n",
    "cv = ShuffleSplit(n_splits=5, test_size=1/5, random_state=4)\n",
    "# cv could be a fixed number of partitions but there would be no shuffling in that case\n",
    "# it will just rotate on partitions (k-1) parts and 1 part for cross-val\n",
    "cv_results_forest = cross_validate(rfr, X, y=y, cv=cv,\\\n",
    "                    scoring = 'neg_mean_squared_error', n_jobs = -1, verbose = 1)\n",
    "print('MSE (mean cross-validation) = {:.4f}'.format(\\\n",
    "                                    -np.mean(cv_results_forest['test_score'])))\n",
    "print('RMSE (mean cross-validation) = {:.4f}'.format(\\\n",
    "                                    np.sqrt(-np.mean(cv_results_forest['test_score']))))\n",
    "# 500 trees => RSME 2633.5295, 8h\n",
    "# 20 trees => RSME 2635.2640, 20 min\n",
    "# 10 trees => RSME 2637\n",
    "\n",
    "# NOTE: Out-Of-Bag samples only tells you about the performance of each tree on aggregate and\n",
    "# not on the entire random forest, it won't be a very good indicator.\n",
    "rfr.fit(X,y)\n",
    "print('Training Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# feature importance shows Product categories are the best indicator followed by Product ID\n",
    "# and user ID (barely)\n",
    "# _, axrf = plt.subplots()\n",
    "# axrf.plot(rfr.feature_importances_)\n",
    "# rfr.estimators_\n",
    "# SAVE MODEL\n",
    "# from sklearn.externals import joblib\n",
    "# joblib.dump(rfr, 'RandomForest_500_Model.pkl')\n",
    "#rfr = joblib.load('RandomForest_500_Model.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
